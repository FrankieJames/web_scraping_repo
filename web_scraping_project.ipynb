{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "====== WebDriver manager ======\n",
      "Current google-chrome version is 101.0.4951\n",
      "Get LATEST chromedriver version for 101.0.4951 google-chrome\n",
      "Driver [/Users/frankiejames/.wdm/drivers/chromedriver/mac64_m1/101.0.4951.41/chromedriver] found in cache\n",
      "/var/folders/sf/lnrr_g5s6bd9cf8zjnzr2b6w0000gn/T/ipykernel_53303/3171365371.py:27: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  self.driver = Chrome(ChromeDriverManager().install())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "article container is <selenium.webdriver.remote.webelement.WebElement (session=\"3f8b012cd065e74b43b1388cfa277d28\", element=\"d111c8c7-7ec4-4677-8f64-5f7eb673468c\")>\n",
      "There are 40 articles on this page\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'title_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/frankiejames/Documents/GitHub/web_scraping_repo/web_scraping_project.ipynb Cell 1'\u001b[0m in \u001b[0;36m<cell line: 132>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/frankiejames/Documents/GitHub/web_scraping_repo/web_scraping_project.ipynb#ch0000000?line=133'>134</a>\u001b[0m \u001b[39m# news_scraper.scroll_down()\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/frankiejames/Documents/GitHub/web_scraping_repo/web_scraping_project.ipynb#ch0000000?line=134'>135</a>\u001b[0m news_scraper\u001b[39m.\u001b[39mnews_button()\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/frankiejames/Documents/GitHub/web_scraping_repo/web_scraping_project.ipynb#ch0000000?line=135'>136</a>\u001b[0m news_scraper\u001b[39m.\u001b[39;49mlist_of_articles()\n",
      "\u001b[1;32m/Users/frankiejames/Documents/GitHub/web_scraping_repo/web_scraping_project.ipynb Cell 1'\u001b[0m in \u001b[0;36mNews_Scraper.list_of_articles\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/frankiejames/Documents/GitHub/web_scraping_repo/web_scraping_project.ipynb#ch0000000?line=77'>78</a>\u001b[0m \u001b[39mfor\u001b[39;00m link \u001b[39min\u001b[39;00m link_list:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/frankiejames/Documents/GitHub/web_scraping_repo/web_scraping_project.ipynb#ch0000000?line=78'>79</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdriver\u001b[39m.\u001b[39mget(link)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/frankiejames/Documents/GitHub/web_scraping_repo/web_scraping_project.ipynb#ch0000000?line=79'>80</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_info(title_list, author_list, date_list, source_list)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/frankiejames/Documents/GitHub/web_scraping_repo/web_scraping_project.ipynb#ch0000000?line=80'>81</a>\u001b[0m     \u001b[39m# self.ID_for_each_article()\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/frankiejames/Documents/GitHub/web_scraping_repo/web_scraping_project.ipynb#ch0000000?line=82'>83</a>\u001b[0m \u001b[39mprint\u001b[39m(link_list)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'title_list' is not defined"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver import Chrome \n",
    "#from selenium.webdriver.chrome import ChromeDriverManager\n",
    "from webdriver_manager.chrome import ChromeDriverManager \n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support import expected_conditions as EC \n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time \n",
    "import os \n",
    "import uuid \n",
    "import json \n",
    "\n",
    "\n",
    "class News_Scraper:\n",
    "\n",
    "    def __init__(self, url) -> None:\n",
    "        self.url = url \n",
    "        self.article_links = []\n",
    "        self.article_data = {\n",
    "            \"title\": [],\n",
    "            \"author\": [],\n",
    "            \"date\": [],\n",
    "            \"source\": [],\n",
    "            \"uuid\": []\n",
    "        }\n",
    "\n",
    "        self.driver = Chrome(ChromeDriverManager().install())\n",
    "        self.driver.get(url)\n",
    "        time.sleep(2)\n",
    "\n",
    "        try:\n",
    "            click_accept_cookies = self.driver.find_element(By.ID, \"cookie-accept-link-text\")\n",
    "            click_accept_cookies.click()\n",
    "            time.sleep(2)\n",
    "        except:\n",
    "            print(\"Couldn't accept cookies, or there were none to accept\")\n",
    "            pass \n",
    "        return\n",
    "\n",
    "    # def scroll_down(self):\n",
    "    #     try:\n",
    "    #         popular_articles = self.driver.find_element(By.TAG_NAME, 'body') \n",
    "    #         popular_articles.send_keys(Keys.END)\n",
    "    #         time.sleep(2)\n",
    "    #     except:\n",
    "    #         print(\"Didn't scroll to Medical Links\")\n",
    "    #         pass \n",
    "    #     return\n",
    "            \n",
    "    def news_button(self):\n",
    "        try:\n",
    "            time.sleep(2)\n",
    "            click_news_button = self.driver.find_element(By.LINK_TEXT, \"News\")\n",
    "            click_news_button.click()\n",
    "            time.sleep(2)\n",
    "        except:\n",
    "            print(\"Didn't click 'News' button\")\n",
    "            pass \n",
    "        return  \n",
    "    \n",
    "    def list_of_articles(self):\n",
    "        time.sleep(2)\n",
    "        article_container = self.driver.find_element(By.XPATH, '//div[@class=\"row\"]')\n",
    "        print(f\"article container is {article_container}\")\n",
    "        #article_list = article_container.find_elements(By.XPATH, './div')\n",
    "        #print(f\"article list is {article_list}\")\n",
    "        \n",
    "        link_list = []\n",
    "        today = time.strftime('%Y%m%d')\n",
    "        url = self.driver.find_elements(By.XPATH, f\"//a[contains (@href, 'news/{today}')]\")\n",
    "        # Change ending to today's date. \n",
    "        for item in url:\n",
    "            href = item.get_attribute('href')\n",
    "            link_list.append(href)\n",
    "            #print(href)\n",
    "        print(f'There are {len(link_list)} articles on this page')\n",
    "\n",
    "        for link in link_list:\n",
    "            self.driver.get(link)\n",
    "            self.get_info(title_list, author_list, date_list, source_list)\n",
    "            # self.ID_for_each_article()\n",
    "\n",
    "        print(link_list)\n",
    "        return link_list\n",
    "\n",
    "    def list_of_titles(self):\n",
    "        \"\"\" List of the titles of each article from the news page\"\"\"\n",
    "        title_list = self.driver.find_elements(By.CLASS_NAME, \"hidden-xs item-desc\")\n",
    "\n",
    "    def get_info(self, title_list, author_list, date_list, source_list):\n",
    "        \"\"\"\n",
    "        Get the key information from each article (after following each link to seperate article)\n",
    "        \"\"\"\n",
    "        title_list = self.driver.find_elements(By.CLASS_NAME, \"page-title\")\n",
    "        print(f\"Title list has type {type(title_list)} and length {len(title_list)})\")\n",
    "        author_list = self.driver.find_elements(By.CLASS_NAME, \"article-meta-reviewer\")\n",
    "        print(f\"Author list has type {type(author_list)} and length {len(author_list)})\")\n",
    "        date_list = self.driver.find_elements(By.CLASS_NAME, \"article-meta-date\")\n",
    "        print(f\"Date list has type {type(date_list)} and length {len(date_list)})\")\n",
    "        source_list = self.driver.find_elements(By.CLASS_NAME, \"content-src-value\")\n",
    "        print(f\"Source list has type {type(source_list)} and length {len(source_list)})\")\n",
    "        #uuid_list = self.ID_for_each_article()\n",
    "        # print(f\"uuid list has type {type(uuid_list)} and length {len(uuid_list)})\")\n",
    "        # print(f\"uuid list has type {type(uuid)} and lenght {len(uuid_list)})\")\n",
    "        self.add_info(title_list, author_list, date_list, source_list)\n",
    "        #list_of_links_to_related_stories =\n",
    "        #quoted =\n",
    "\n",
    "    def add_info(self, title_list, author_list, date_list, source_list):\n",
    "        self.article_data[\"title\"] = (self.article_data[\"title\"] +[item.text for item in title_list])\n",
    "        self.article_data[\"author\"] = (self.article_data[\"author\"] +[item.text for item in author_list])\n",
    "        self.article_data[\"date\"] = (self.article_data[\"date\"] +[item.text for item in date_list])\n",
    "        self.article_data[\"source\"] = (self.article_data[\"source\"] +[item.text for item in source_list])\n",
    "        # self.article_data[\"uuid\"] = (self.article_data[\"uuid\"] + [item.text for item in uuid_list])\n",
    "        return\n",
    "\n",
    "    # def create_folder(self, folder):\n",
    "    #     if not os.path.exists(folder):\n",
    "    #         os.makedirs(folder)\n",
    "\n",
    "    # def ID_for_each_article(self):\n",
    "    #     \"\"\" \n",
    "    #     Create a list of uuid for each item in a list (i.e. for each url in the link_list, create a associated uuid\n",
    "    #     Using version 4 \n",
    "    #     \"\"\"\n",
    "    #     uuid_list = []\n",
    "    #     self.article_data[\"uuid\"]\n",
    "    #     link_ID = uuid.uuid4()\n",
    "    #     uuid_list.append(link_ID)\n",
    "    #     return uuid_list\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    news_scraper = News_Scraper('https://www.news-medical.net/')\n",
    "    # news_scraper.scroll_down()\n",
    "    news_scraper.news_button()\n",
    "    news_scraper.list_of_articles()\n",
    "    # for item in link_list:\n",
    "    #     news_scraper.\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "eebf51706ff08164fbfc0f7261be8d269ebf4f841796856fd2cb500d849fc298"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 ('wbs_env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
